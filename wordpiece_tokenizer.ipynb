{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUZV41nKgs2P",
        "outputId": "c368499a-dfd4-42fd-9ed1-b6251226976f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting Tokenizer\n",
            "  Downloading tokenizer-3.4.3-py2.py3-none-any.whl.metadata (42 kB)\n",
            "     ---------------------------------------- 0.0/42.2 kB ? eta -:--:--\n",
            "     --------- ------------------------------ 10.2/42.2 kB ? eta -:--:--\n",
            "     ------------------ ------------------- 20.5/42.2 kB 320.0 kB/s eta 0:00:01\n",
            "     ------------------------------------ - 41.0/42.2 kB 326.8 kB/s eta 0:00:01\n",
            "     -------------------------------------- 42.2/42.2 kB 255.7 kB/s eta 0:00:00\n",
            "Downloading tokenizer-3.4.3-py2.py3-none-any.whl (112 kB)\n",
            "   ---------------------------------------- 0.0/112.3 kB ? eta -:--:--\n",
            "   -------------- ------------------------- 41.0/112.3 kB 2.0 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 81.9/112.3 kB 1.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 112.3/112.3 kB 1.1 MB/s eta 0:00:00\n",
            "Installing collected packages: Tokenizer\n",
            "Successfully installed Tokenizer-3.4.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWpxHYErh-l3",
        "outputId": "110b58c0-91a0-4bce-cb9a-dde13ba03ab5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\kaout\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\kaout\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kaout\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kaout\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kaout\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\kaout\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_VamxcPh-l4",
        "outputId": "58959740-a475-40e5-f4bb-32e2ce2939e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.19.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers)\n",
            "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers)\n",
            "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers)\n",
            "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\kaout\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kaout\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: requests in c:\\users\\kaout\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\kaout\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\n",
            "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.16.4->tokenizers)\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\kaout\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kaout\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kaout\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kaout\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kaout\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.2)\n",
            "Downloading tokenizers-0.19.1-cp312-none-win_amd64.whl (2.2 MB)\n",
            "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.1/2.2 MB 777.7 kB/s eta 0:00:03\n",
            "   -- ------------------------------------- 0.2/2.2 MB 1.1 MB/s eta 0:00:02\n",
            "   ---- ----------------------------------- 0.2/2.2 MB 1.5 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 0.5/2.2 MB 2.3 MB/s eta 0:00:01\n",
            "   ---------- ----------------------------- 0.6/2.2 MB 2.1 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 0.7/2.2 MB 2.4 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 1.0/2.2 MB 2.7 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 1.1/2.2 MB 2.8 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 1.3/2.2 MB 2.9 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 1.5/2.2 MB 2.9 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 1.8/2.2 MB 3.1 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 1.9/2.2 MB 3.1 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 2.1/2.2 MB 3.2 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 2.1/2.2 MB 3.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  2.2/2.2 MB 3.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.2/2.2 MB 2.9 MB/s eta 0:00:00\n",
            "Downloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
            "   ---------------------------------------- 0.0/402.6 kB ? eta -:--:--\n",
            "   ------- -------------------------------- 71.7/402.6 kB 1.3 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 194.6/402.6 kB 2.4 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 286.7/402.6 kB 2.2 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 286.7/402.6 kB 2.2 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 286.7/402.6 kB 2.2 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 286.7/402.6 kB 2.2 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 286.7/402.6 kB 2.2 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 286.7/402.6 kB 2.2 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 286.7/402.6 kB 2.2 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 286.7/402.6 kB 2.2 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 286.7/402.6 kB 2.2 MB/s eta 0:00:01\n",
            "   ------------------------------------ - 389.1/402.6 kB 692.6 kB/s eta 0:00:01\n",
            "   -------------------------------------- 402.6/402.6 kB 678.9 kB/s eta 0:00:00\n",
            "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
            "   ---------------------------------------- 0.0/177.6 kB ? eta -:--:--\n",
            "   ------------- -------------------------- 61.4/177.6 kB 1.7 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 92.2/177.6 kB 1.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------  174.1/177.6 kB 1.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 177.6/177.6 kB 1.2 MB/s eta 0:00:00\n",
            "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: typing-extensions, fsspec, filelock, huggingface-hub, tokenizers\n",
            "Successfully installed filelock-3.15.4 fsspec-2024.6.1 huggingface-hub-0.23.4 tokenizers-0.19.1 typing-extensions-4.12.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTchkm8ah-l4",
        "outputId": "4fbe1e99-34af-4c90-bc96-28f26b3e918e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: tokenizersNote: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Version: 0.19.1\n",
            "Summary: \n",
            "Home-page: \n",
            "Author: Anthony MOI <m.anthony.moi@gmail.com>\n",
            "Author-email: Nicolas Patry <patry.nicolas@protonmail.com>, Anthony Moi <anthony@huggingface.co>\n",
            "License: \n",
            "Location: C:\\Users\\kaout\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\n",
            "Requires: huggingface-hub\n",
            "Required-by: transformers\n"
          ]
        }
      ],
      "source": [
        "pip show tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZCL4rBzqDFn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tpipv-XJCzO"
      },
      "outputs": [],
      "source": [
        "small_data=pd.read_csv('GCF_000441575.1_ASM44157v1_genomic.fna')\n",
        "bigger_data=pd.read_csv('GCF_000146045.2_R64_genomic.fna')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIAGuzA9h-l5",
        "outputId": "2e57ca85-97d1-4030-fe84-51de839444dd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>&gt;NC_021894.1 Candidatus Carsonella ruddii DC</th>\n",
              "      <th>complete sequence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ATGAAAAATATTATTGTTGCAAAAGTTACTCCTGATGATTTAACAT...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAGAAAATTTATAAAACCTTTAATTAAAAAAAAATTAAAGATTCAA...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ATCAATTTATAGACTTTGTGTTAGTAGTGTTTTTCAAATCACCTAA...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TTACACGGTAATTTATATTTGTCAACAATAATAATGGAATTTTTGA...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ATTTTTAGAGAGAAGATATTTGAATGGTAAAATTTCTTTAATGGAA...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2171</th>\n",
              "      <td>ATTAATAATAAATTACAATTTTTAATTAAATTTTTCTTTAAAAAAT...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2172</th>\n",
              "      <td>TTTAAAAAAAACCAAAGAAATTAAATTGATATCATGTTTAATAAAA...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2173</th>\n",
              "      <td>TATAATTTATATTATATTTTTTTAAAAAGTTCAATAAATAATTTAT...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2174</th>\n",
              "      <td>AGTTTTTTTTTGTTTAAATAATTTATTGTCTTAATATCGTTCATAG...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2175</th>\n",
              "      <td>TTTTTTAAATAAAA</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2176 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           >NC_021894.1 Candidatus Carsonella ruddii DC   complete sequence\n",
              "0     ATGAAAAATATTATTGTTGCAAAAGTTACTCCTGATGATTTAACAT...                 NaN\n",
              "1     AAGAAAATTTATAAAACCTTTAATTAAAAAAAAATTAAAGATTCAA...                 NaN\n",
              "2     ATCAATTTATAGACTTTGTGTTAGTAGTGTTTTTCAAATCACCTAA...                 NaN\n",
              "3     TTACACGGTAATTTATATTTGTCAACAATAATAATGGAATTTTTGA...                 NaN\n",
              "4     ATTTTTAGAGAGAAGATATTTGAATGGTAAAATTTCTTTAATGGAA...                 NaN\n",
              "...                                                 ...                 ...\n",
              "2171  ATTAATAATAAATTACAATTTTTAATTAAATTTTTCTTTAAAAAAT...                 NaN\n",
              "2172  TTTAAAAAAAACCAAAGAAATTAAATTGATATCATGTTTAATAAAA...                 NaN\n",
              "2173  TATAATTTATATTATATTTTTTTAAAAAGTTCAATAAATAATTTAT...                 NaN\n",
              "2174  AGTTTTTTTTTGTTTAAATAATTTATTGTCTTAATATCGTTCATAG...                 NaN\n",
              "2175                                     TTTTTTAAATAAAA                 NaN\n",
              "\n",
              "[2176 rows x 2 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "small_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAstvq7bh-l5",
        "outputId": "ed28ce1a-b97c-465f-dbdd-8f20c70f109d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>&gt;NC_001133.9 Saccharomyces cerevisiae S288C chromosome I</th>\n",
              "      <th>complete sequence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ccacaccacacccacacacccacacaccacaccacacaccacacca...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ACAGCCCTAATCTAACCCTGGCCAACCTGTCTCTCAACTTACCCTC...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TCAACCATACCACTCCGAACCACCATCCATCCCTCTACTTACTACC...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CAACCCACTGCCACTTACCCTACCATTACCCTACCATCCACCATGA...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TGAAACGCTAACAAATGATCGTAAATAACACACACGTGCTTACCCT...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151984</th>\n",
              "      <td>ATATTTATAAAGAAATATATATAAAAAGTCATTATATAAATTTAAT...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151985</th>\n",
              "      <td>ATATTAATAAAGTAATTAGTATAAATAAATAATATGAAAATAAAAC...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151986</th>\n",
              "      <td>CCCGCGGCGGGCGGACCCCGAAGGAGTGAGGGACCCCTCCCTAATG...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151987</th>\n",
              "      <td>CATATATATATATTAATAAAAAAAAGTAATATATATATATATATTG...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151988</th>\n",
              "      <td>TTATAATATAATATCCATA</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>151989 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       >NC_001133.9 Saccharomyces cerevisiae S288C chromosome I  \\\n",
              "0       ccacaccacacccacacacccacacaccacaccacacaccacacca...         \n",
              "1       ACAGCCCTAATCTAACCCTGGCCAACCTGTCTCTCAACTTACCCTC...         \n",
              "2       TCAACCATACCACTCCGAACCACCATCCATCCCTCTACTTACTACC...         \n",
              "3       CAACCCACTGCCACTTACCCTACCATTACCCTACCATCCACCATGA...         \n",
              "4       TGAAACGCTAACAAATGATCGTAAATAACACACACGTGCTTACCCT...         \n",
              "...                                                   ...         \n",
              "151984  ATATTTATAAAGAAATATATATAAAAAGTCATTATATAAATTTAAT...         \n",
              "151985  ATATTAATAAAGTAATTAGTATAAATAAATAATATGAAAATAAAAC...         \n",
              "151986  CCCGCGGCGGGCGGACCCCGAAGGAGTGAGGGACCCCTCCCTAATG...         \n",
              "151987  CATATATATATATTAATAAAAAAAAGTAATATATATATATATATTG...         \n",
              "151988                                TTATAATATAATATCCATA         \n",
              "\n",
              "        complete sequence  \n",
              "0                     NaN  \n",
              "1                     NaN  \n",
              "2                     NaN  \n",
              "3                     NaN  \n",
              "4                     NaN  \n",
              "...                   ...  \n",
              "151984                NaN  \n",
              "151985                NaN  \n",
              "151986                NaN  \n",
              "151987                NaN  \n",
              "151988                NaN  \n",
              "\n",
              "[151989 rows x 2 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigger_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sAzW5Gnm86I",
        "outputId": "dadbc352-8d23-4176-c137-e839f532c3e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['>NC_021894.1 Candidatus Carsonella ruddii DC', ' complete sequence'], dtype='object')\n",
            "Index(['>NC_001133.9 Saccharomyces cerevisiae S288C chromosome I', ' complete sequence'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(small_data.columns)\n",
        "print(bigger_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKTk27cmnyn7",
        "outputId": "f04b2329-e817-4eb7-b060-9edb9f68c74a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Candidatus Carsonella ruddii geneome sequence length:  176189\n",
            "Saccharomyces cerevisiae S288C chromosome I genome sequence length: 12310013\n"
          ]
        }
      ],
      "source": [
        "merged_text1 = ' '.join(small_data['>NC_021894.1 Candidatus Carsonella ruddii DC'])\n",
        "merged_text2 = ' '.join(bigger_data['>NC_001133.9 Saccharomyces cerevisiae S288C chromosome I'])\n",
        "print('Candidatus Carsonella ruddii geneome sequence length: ',len(merged_text1))\n",
        "print('Saccharomyces cerevisiae S288C chromosome I genome sequence length:',len(merged_text2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MCY3LL_pHS9"
      },
      "outputs": [],
      "source": [
        "genome_file_path1='Candidatus_Carsonella_ruddii_DC.txt'\n",
        "genome_file_path2='Saccharomyces_cerevisiae_S288C_chromosome I.txt'\n",
        "with open(genome_file_path1, 'w') as file:\n",
        "  file.write(merged_text1)\n",
        "with open(genome_file_path2, 'w') as file:\n",
        "  file.write(merged_text2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDTLNkOC1wq-"
      },
      "outputs": [],
      "source": [
        "# Initialize a WordPiece tokenizer that will be trained according to the genome sequence chosen\n",
        "def train_tokenizer(data_file,tokenizer_config_path):\n",
        "  x=20000\n",
        "  tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\",\n",
        "                            max_input_chars_per_word=10**15))\n",
        "  tokenizer.vocab_size=x\n",
        "  tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "  tokenizer.decoder = decoders.WordPiece()\n",
        "\n",
        "  tokenizer.train(files=[data_file])\n",
        "\n",
        "  tokenizer.save(tokenizer_config_path)\n",
        "  return tokenizer_config_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-yQmr6cl_q9"
      },
      "outputs": [],
      "source": [
        "#using wordpiece tokenization\n",
        "def tokenize_file(file_path, tokenizer, batch_size=5000):\n",
        "    tokens = []\n",
        "    ids = []\n",
        "    vocab_set = set(tokenizer.get_vocab().keys())\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        while True:\n",
        "            batch = file.read(batch_size)\n",
        "            if not batch:\n",
        "                break\n",
        "            encoded = tokenizer.encode(batch)\n",
        "            tokens.extend(encoded.tokens)\n",
        "            ids.extend(encoded.ids)\n",
        "\n",
        "    tokens = [token if token in vocab_set else '[UNK]' for token in tokens]\n",
        "\n",
        "    return tokens, ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boj8D_kcVKVg"
      },
      "outputs": [],
      "source": [
        "tokenizer_config_path1 = \"trained_tokenizer.json\"\n",
        "tokenizer_config_path1 = train_tokenizer(genome_file_path1, tokenizer_config_path1)\n",
        "tokenizer1 = Tokenizer.from_file(tokenizer_config_path1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPN9q4H7h-l7",
        "outputId": "24709899-597f-43d3-dd51-090e5d4e79e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "tokenizer1.get_vocab_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcmTb5ZfeTwc"
      },
      "outputs": [],
      "source": [
        "tokenizer_config_path2 = \"trained_tokenizer1.json\"\n",
        "tokenizer_config_path2=train_tokenizer(genome_file_path2,tokenizer_config_path2)\n",
        "tokenizer2 = Tokenizer.from_file(tokenizer_config_path2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHcDHR5C7CaE",
        "outputId": "21b69e7d-cf85-4afc-8b19-bb970df4431c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['ATGAAAAATATTATTGTTGCAAAAGTTACTCCTGATGATTTAACATCAATTTGCATTATTAGATTATCTGGAAAAAAATT', 'AAGAAAATTTATAAAACCTTTAATTAAAAAAAAATTAAAGATTCAAAAATTAGAATATACAAAATTATATGGATTAAATA', 'ATCAATTTATAGACTTTGTGTTAGTAGTGTTTTTCAAATCACCTAAATCATTAACAGGTGAAGATGTTATCGAGTTT', '##CAT', 'TTACACGGTAATTTATATTTGTCAACAATAATAATGGAATTTTTGATTAAGTTAGGAGCTAAAATTGCAAAACCAGGGGA', 'ATTTTTAGAGAGAAGATATTTGAATGGTAAAATTTCTTTAATGGAATGTGAAATGATTAATAATAAAATTATTTATAATA', 'ATGAGAATATGTTTAAGTTAACTTCAAATTCTGAAAAAGATATATATCTTTGTATAATTAAAAATTCAAGATTTAGAATA', 'AATATGCTAATTATTTGTTTAGAATTTATTTTAATAAATGAAAAAGAATCGTTATTAAGAGATTTTATTTTTATAAAAAA', 'ATTTTTAAAAAAATTTAAAAATTTTATAAATATATTATTAAAAAAAATATCAAAAATTAAATATTTTAAAAAAATATTTG', 'AAATTATGATAATGGGTAGACGAAATGTTGGGAAATCAACTTTATTTAATAAATTATGTTTACAATATGATTCTATTGTA', 'ACAAATATACCTGGAACAACAACAAATACGATTACTAAACAAATATATTTTACTTCAAAAACAATTAATTTAAACGATAC', 'AGCAGGTTTAAAAATAAAAACAAAAAATTTAATAGAAAAAATTGGTATAATGAAAAATATCAATAAATCTTACGAAGGAA', 'ATTTAGTTCTTTATATTATTGATAAATTTGATTTAAGAAGAGTATTTTATAATACTCCATTAGATTTTTTTGATAAAATA', 'AAAAGCAATGAATTAATTATTATAGTTAATAAATGTGATATTTTTGGAATAAAAGAAGGAATTTATAAAGTAAAAAATTT', 'ATTTGTTATATTTTTATCTGCTAAACATAGTGTTTTAATTTTTAGATTAAAATGTTTTATTAGCAAAATAATAGAAAATG', 'AAAAATTATTGTTTAATAATAATATGAATTATGAAAATGTAAAATTATTATTTAATGAATGTTCAATTTTTTATAAAGAA', 'TTTTGTTGTAATTACGATATTATATTAGAAAAAGTAATAAAATTTCAAAAAAATGTATTAAAAATGACTGGCGAATATAC', 'TAATAATTATATTCTAAATTCTTTATTTAGTAATTTTTGTATGGGGAAATAATGATTTATAATGTTATTGTAATTGGTTC', 'AGGACACGCTGGAATTGAAGCTGTATGTGTTTCTTCTAAAATATGTAATAAAGTTAAATTAATTACTTCAAATATAGAAA', 'GTATTGGAATACTATCATGTAATCCATCAATAGGAGGAATTGGTAAAAGTCATATTGTTCAAGAATTAGAAATATTAGGA']\n",
            "Token IDs: [28395, 29983, 28430, 474, 28248, 29108, 28453, 29148, 29098, 29042, 29382, 26209, 28871, 29447, 28864, 29880, 28732, 29762, 29280, 28812]\n",
            "Time taken: 0.06872844696044922\n",
            "Number of tokens: 2801\n"
          ]
        }
      ],
      "source": [
        "#short genome\n",
        "start = time.time()\n",
        "tokens, ids = tokenize_file(genome_file_path1, tokenizer1)\n",
        "end = time.time()\n",
        "\n",
        "print(\"Tokens:\", tokens[:20])\n",
        "print(\"Token IDs:\", ids[:20])\n",
        "print(\"Time taken:\", end - start)\n",
        "print(\"Number of tokens:\", len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0N2e_815Cm1",
        "outputId": "d223f688-a427-4c2a-9ff5-dae5668b940d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['ccac', '##acca', '##caccc', '##acacacc', '##caca', '##cacc', '##acacc', '##acacacc', '##acacc', '##acacccacac', '##acaca', '##cat', '##CCTAAC', '##ACTACCC', '##TAAC', 'ACAGC', '##CCTAA', '##TCTAACC', '##C', '##TGGCCAAC']\n",
            "Token IDs: [22033, 3658, 17314, 28617, 9211, 6475, 4426, 28617, 4426, 5191, 19944, 3331, 14637, 5452, 146, 7807, 2557, 7331, 40, 21631]\n",
            "Time taken: 18.33873462677002\n",
            "Number of tokens: 2089468\n"
          ]
        }
      ],
      "source": [
        "#long genome\n",
        "start = time.time()\n",
        "tokens, ids = tokenize_file(genome_file_path2, tokenizer2)\n",
        "end = time.time()\n",
        "\n",
        "print(\"Tokens:\", tokens[:20])\n",
        "print(\"Token IDs:\", ids[:20])\n",
        "print(\"Time taken:\", end - start)\n",
        "print(\"Number of tokens:\", len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fr3gkukqnwuy"
      },
      "outputs": [],
      "source": [
        "#using binary search to split data\n",
        "def tokenize_file_binary_search(file_path, tokenizer, batch_size=5000):\n",
        "    tokens = []\n",
        "    ids = []\n",
        "    vocab_set = set(tokenizer.get_vocab().keys())\n",
        "\n",
        "    def tokenize_batch(text):\n",
        "        encoded = tokenizer.encode(text)\n",
        "        return [token if token in vocab_set else '[UNK]' for token in encoded.tokens], encoded.ids\n",
        "\n",
        "    # Function to perform binary search for split points\n",
        "    def find_split_points(file_obj, batch_size):\n",
        "        file_obj.seek(0, 2)\n",
        "        file_size = file_obj.tell()\n",
        "        start = 0\n",
        "        split_points = [0]\n",
        "\n",
        "        while start < file_size:\n",
        "            mid = (start + file_size) // 2\n",
        "            file_obj.seek(mid)\n",
        "            file_obj.readline()\n",
        "            split_points.append(file_obj.tell())\n",
        "            start = file_obj.tell()\n",
        "\n",
        "        return split_points\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        split_points = find_split_points(file, batch_size)\n",
        "\n",
        "        for i in range(len(split_points) - 1):\n",
        "            file.seek(split_points[i])\n",
        "            chunk = file.read(split_points[i+1] - split_points[i])\n",
        "            batch_tokens, batch_ids = tokenize_batch(chunk)\n",
        "            tokens.extend(batch_tokens)\n",
        "            ids.extend(batch_ids)\n",
        "\n",
        "    return tokens, ids\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG3zahyIV8im",
        "outputId": "3ce65b37-c606-4de0-fdff-f192015b82ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['ATGAAAAATATTATTGTTGCAAAAGTTACTCCTGATGATTTAACATCAATTTGCATTATTAGATTATCTGGAAAAAAATT', 'AAGAAAATTTATAAAACCTTTAATTAAAAAAAAATTAAAGATTCAAAAATTAGAATATACAAAATTATATGGATTAAATA', 'ATCAATTTATAGACTTTGTGTTAGTAGTGTTTTTCAAATCACCTAAATCATTAACAGGTGAAGATGTTATCGAGTTT', '##CAT', 'TTACACGGTAATTTATATTTGTCAACAATAATAATGGAATTTTTGATTAAGTTAGGAGCTAAAATTGCAAAACCAGGGGA', 'ATTTTTAGAGAGAAGATATTTGAATGGTAAAATTTCTTTAATGGAATGTGAAATGATTAATAATAAAATTATTTATAATA', 'ATGAGAATATGTTTAAGTTAACTTCAAATTCTGAAAAAGATATATATCTTTGTATAATTAAAAATTCAAGATTTAGAATA', 'AATATGCTAATTATTTGTTTAGAATTTATTTTAATAAATGAAAAAGAATCGTTATTAAGAGATTTTATTTTTATAAAAAA', 'ATTTTTAAAAAAATTTAAAAATTTTATAAATATATTATTAAAAAAAATATCAAAAATTAAATATTTTAAAAAAATATTTG', 'AAATTATGATAATGGGTAGACGAAATGTTGGGAAATCAACTTTATTTAATAAATTATGTTTACAATATGATTCTATTGTA', 'ACAAATATACCTGGAACAACAACAAATACGATTACTAAACAAATATATTTTACTTCAAAAACAATTAATTTAAACGATAC', 'AGCAGGTTTAAAAATAAAAACAAAAAATTTAATAGAAAAAATTGGTATAATGAAAAATATCAATAAATCTTACGAAGGAA', 'ATTTAGTTCTTTATATTATTGATAAATTTGATTTAAGAAGAGTATTTTATAATACTCCATTAGATTTTTTTGATAAAATA', 'AAAAGCAATGAATTAATTATTATAGTTAATAAATGTGATATTTTTGGAATAAAAGAAGGAATTTATAAAGTAAAAAATTT', 'ATTTGTTATATTTTTATCTGCTAAACATAGTGTTTTAATTTTTAGATTAAAATGTTTTATTAGCAAAATAATAGAAAATG', 'AAAAATTATTGTTTAATAATAATATGAATTATGAAAATGTAAAATTATTATTTAATGAATGTTCAATTTTTTATAAAGAA', 'TTTTGTTGTAATTACGATATTATATTAGAAAAAGTAATAAAATTTCAAAAAAATGTATTAAAAATGACTGGCGAATATAC', 'TAATAATTATATTCTAAATTCTTTATTTAGTAATTTTTGTATGGGGAAATAATGATTTATAATGTTATTGTAATTGGTTC', 'AGGACACGCTGGAATTGAAGCTGTATGTGTTTCTTCTAAAATATGTAATAAAGTTAAATTAATTACTTCAAATATAGAAA', 'GTATTGGAATACTATCATGTAATCCATCAATAGGAGGAATTGGTAAAAGTCATATTGTTCAAGAATTAGAAATATTAGGA']\n",
            "Token IDs: [28395, 29983, 28430, 474, 28248, 29108, 28453, 29148, 29098, 29042, 29382, 26209, 28871, 29447, 28864, 29880, 28732, 29762, 29280, 28812]\n",
            "Time taken: 0.0898439884185791\n",
            "Number of tokens: 2608\n"
          ]
        }
      ],
      "source": [
        "#short genome\n",
        "start = time.time()\n",
        "tokens, ids = tokenize_file_binary_search(genome_file_path1, tokenizer1)\n",
        "end = time.time()\n",
        "\n",
        "print(\"Tokens:\", tokens[:20])\n",
        "print(\"Token IDs:\", ids[:20])\n",
        "print(\"Time taken:\", end - start)\n",
        "print(\"Number of tokens:\", len(tokens))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xco91EKWWG0e",
        "outputId": "49c9e3c6-25c7-463b-98aa-25c314cc68df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['ccac', '##acca', '##caccc', '##acacacc', '##caca', '##cacc', '##acacc', '##acacacc', '##acacc', '##acacccacac', '##acaca', '##cat', '##CCTAAC', '##ACTACCC', '##TAAC', 'ACAGC', '##CCTAA', '##TCTAACC', '##C', '##TGGCCAAC']\n",
            "Token IDs: [22033, 3658, 17314, 28617, 9211, 6475, 4426, 28617, 4426, 5191, 19944, 3331, 14637, 5452, 146, 7807, 2557, 7331, 40, 21631]\n",
            "Time taken: 22.325748443603516\n",
            "Number of tokens: 2087538\n"
          ]
        }
      ],
      "source": [
        "#long genome\n",
        "start = time.time()\n",
        "tokens, ids = tokenize_file_binary_search(genome_file_path2, tokenizer2)\n",
        "end = time.time()\n",
        "\n",
        "print(\"Tokens:\", tokens[:20])\n",
        "print(\"Token IDs:\", ids[:20])\n",
        "print(\"Time taken:\", end - start)\n",
        "print(\"Number of tokens:\", len(tokens))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4whMmbnepHAA"
      },
      "outputs": [],
      "source": [
        "#Using linear search algorithm\n",
        "def tokenize_file_linear_search(file_path, tokenizer, chunk_size=5000):\n",
        "    tokens = []\n",
        "    ids = []\n",
        "    vocab_set = set(tokenizer.get_vocab().keys())\n",
        "\n",
        "    # Function to tokenize a batch of text\n",
        "    def tokenize_chunk(chunk):\n",
        "        encoded = tokenizer.encode(chunk)\n",
        "        return [token if token in vocab_set else '[UNK]' for token in encoded.tokens], encoded.ids\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        while True:\n",
        "            chunk = file.read(chunk_size)\n",
        "            if not chunk:\n",
        "                break\n",
        "            batch_tokens, batch_ids = tokenize_chunk(chunk)\n",
        "            tokens.extend(batch_tokens)\n",
        "            ids.extend(batch_ids)\n",
        "\n",
        "    return tokens, ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXoPAJm9WPS4",
        "outputId": "2fbcca63-1de7-4607-9691-e77401eaa317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['ATGAAAAATATTATTGTTGCAAAAGTTACTCCTGATGATTTAACATCAATTTGCATTATTAGATTATCTGGAAAAAAATT', 'AAGAAAATTTATAAAACCTTTAATTAAAAAAAAATTAAAGATTCAAAAATTAGAATATACAAAATTATATGGATTAAATA', 'ATCAATTTATAGACTTTGTGTTAGTAGTGTTTTTCAAATCACCTAAATCATTAACAGGTGAAGATGTTATCGAGTTT', '##CAT', 'TTACACGGTAATTTATATTTGTCAACAATAATAATGGAATTTTTGATTAAGTTAGGAGCTAAAATTGCAAAACCAGGGGA', 'ATTTTTAGAGAGAAGATATTTGAATGGTAAAATTTCTTTAATGGAATGTGAAATGATTAATAATAAAATTATTTATAATA', 'ATGAGAATATGTTTAAGTTAACTTCAAATTCTGAAAAAGATATATATCTTTGTATAATTAAAAATTCAAGATTTAGAATA', 'AATATGCTAATTATTTGTTTAGAATTTATTTTAATAAATGAAAAAGAATCGTTATTAAGAGATTTTATTTTTATAAAAAA', 'ATTTTTAAAAAAATTTAAAAATTTTATAAATATATTATTAAAAAAAATATCAAAAATTAAATATTTTAAAAAAATATTTG', 'AAATTATGATAATGGGTAGACGAAATGTTGGGAAATCAACTTTATTTAATAAATTATGTTTACAATATGATTCTATTGTA', 'ACAAATATACCTGGAACAACAACAAATACGATTACTAAACAAATATATTTTACTTCAAAAACAATTAATTTAAACGATAC', 'AGCAGGTTTAAAAATAAAAACAAAAAATTTAATAGAAAAAATTGGTATAATGAAAAATATCAATAAATCTTACGAAGGAA', 'ATTTAGTTCTTTATATTATTGATAAATTTGATTTAAGAAGAGTATTTTATAATACTCCATTAGATTTTTTTGATAAAATA', 'AAAAGCAATGAATTAATTATTATAGTTAATAAATGTGATATTTTTGGAATAAAAGAAGGAATTTATAAAGTAAAAAATTT', 'ATTTGTTATATTTTTATCTGCTAAACATAGTGTTTTAATTTTTAGATTAAAATGTTTTATTAGCAAAATAATAGAAAATG', 'AAAAATTATTGTTTAATAATAATATGAATTATGAAAATGTAAAATTATTATTTAATGAATGTTCAATTTTTTATAAAGAA', 'TTTTGTTGTAATTACGATATTATATTAGAAAAAGTAATAAAATTTCAAAAAAATGTATTAAAAATGACTGGCGAATATAC', 'TAATAATTATATTCTAAATTCTTTATTTAGTAATTTTTGTATGGGGAAATAATGATTTATAATGTTATTGTAATTGGTTC', 'AGGACACGCTGGAATTGAAGCTGTATGTGTTTCTTCTAAAATATGTAATAAAGTTAAATTAATTACTTCAAATATAGAAA', 'GTATTGGAATACTATCATGTAATCCATCAATAGGAGGAATTGGTAAAAGTCATATTGTTCAAGAATTAGAAATATTAGGA']\n",
            "Token IDs: [28395, 29983, 28430, 474, 28248, 29108, 28453, 29148, 29098, 29042, 29382, 26209, 28871, 29447, 28864, 29880, 28732, 29762, 29280, 28812]\n",
            "Time taken: 0.09010195732116699\n",
            "Number of tokens: 2801\n"
          ]
        }
      ],
      "source": [
        "#short genome\n",
        "start = time.time()\n",
        "tokens, ids = tokenize_file_linear_search(genome_file_path1, tokenizer1)\n",
        "end = time.time()\n",
        "\n",
        "print(\"Tokens:\", tokens[:20])\n",
        "print(\"Token IDs:\", ids[:20])\n",
        "print(\"Time taken:\", end - start)\n",
        "print(\"Number of tokens:\", len(tokens))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XO74ALYWUwc",
        "outputId": "6933f93f-8fdc-4189-adc4-6b75dd5063cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['ccac', '##acca', '##caccc', '##acacacc', '##caca', '##cacc', '##acacc', '##acacacc', '##acacc', '##acacccacac', '##acaca', '##cat', '##CCTAAC', '##ACTACCC', '##TAAC', 'ACAGC', '##CCTAA', '##TCTAACC', '##C', '##TGGCCAAC']\n",
            "Token IDs: [22033, 3658, 17314, 28617, 9211, 6475, 4426, 28617, 4426, 5191, 19944, 3331, 14637, 5452, 146, 7807, 2557, 7331, 40, 21631]\n",
            "Time taken: 18.76608943939209\n",
            "Number of tokens: 2089468\n"
          ]
        }
      ],
      "source": [
        "#long genome\n",
        "start = time.time()\n",
        "tokens, ids = tokenize_file_linear_search(genome_file_path2, tokenizer2)\n",
        "end = time.time()\n",
        "\n",
        "print(\"Tokens:\", tokens[:20])\n",
        "print(\"Token IDs:\", ids[:20])\n",
        "print(\"Time taken:\", end - start)\n",
        "print(\"Number of tokens:\", len(tokens))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X9u6CMHh-l9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}