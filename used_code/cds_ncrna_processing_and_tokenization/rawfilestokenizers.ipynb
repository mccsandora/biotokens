{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import Bio\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_genomes_dir = \"/Users/tiananoll-walker/Documents/biotokens/genome_sequences/raw_genomic_files\"\n",
    "processing_dir = \"/Users/tiananoll-walker/Documents/biotokens/used_code/cds_ncrna_processing_and_tokenization\"\n",
    "subsampled_genomes_dir = os.path.join(processing_dir, \"subsampled_genomes\")\n",
    "tokenizers_dir = os.path.join(processing_dir, \"tokenizers\")\n",
    "chunk_size = 4090\n",
    "default_subsample_size = 10**6\n",
    "\n",
    "os.makedirs(subsampled_genomes_dir, exist_ok=True)\n",
    "os.makedirs(tokenizers_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEBUGGING CELL\n",
    "\n",
    "\n",
    "# print(os.path.getsize('subsampled_genomes/protist_Tetrahymena_thermophila_ncrna.txt'))\n",
    "\n",
    "\n",
    "# file_path = 'subsampled_genomes/protist_Tetrahymena_thermophila_ncrna.txt'\n",
    "\n",
    "# with open(file_path, 'r') as file:\n",
    "#     for i, line in enumerate(file, 1):\n",
    "#         line_length = len(line.strip()) \n",
    "#         print(f\"Line {i}: {line_length} characters\")\n",
    "\n",
    "# with open('subsampled_genomes/protist_Tetrahymena_thermophila_ncrna_subsampled.txt', 'r') as file:\n",
    "#     for i in range(5):\n",
    "#         print(file.readline())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_cds_ncrna(input_file):\n",
    "    \"\"\"merges seqs from a single FASTA file into a single seq\"\"\"\n",
    "    merged_sequence = \"\"\n",
    "    sequence_count = 0\n",
    "    try:\n",
    "        for record in SeqIO.parse(input_file, \"fasta\"):\n",
    "            merged_sequence += str(record.seq)\n",
    "            sequence_count += 1\n",
    "    except Exception as e:\n",
    "        print(f\"issue reading {input_file}: {e}\")\n",
    "        return None, 0\n",
    "\n",
    "    if not merged_sequence:\n",
    "        print(f\"no seqs found in {input_file}\")\n",
    "        return None, 0\n",
    "\n",
    "    print(f\"found {sequence_count} seqs in {input_file}.\")\n",
    "    return merged_sequence, sequence_count\n",
    "\n",
    "def process_genome_into_chunks(genome, path, chunk_size=4096):\n",
    "    chunks = [genome[i:i+chunk_size] + '.' for i in range(0, len(genome), chunk_size)]\n",
    "    with open(path, 'w') as f:\n",
    "        for chunk in chunks:\n",
    "            f.write(chunk + '\\n')\n",
    "    print(f\"processed genome file saved to {path}\")\n",
    "\n",
    "def subsample_genome(genome, size=default_subsample_size):\n",
    "    \"\"\"subsample genome (first and last 500k bases) if genome exceeds the default size 10^6\"\"\"\n",
    "    if len(genome) <= size:\n",
    "        return genome\n",
    "    elif len(genome) <= 2 * size:\n",
    "        return genome\n",
    "    return genome[:size] + genome[-size:]\n",
    "\n",
    "def save_subsampled_genome(org, file_name, subsample):\n",
    "    subsample_path = os.path.join(subsampled_genomes_dir, f\"{org}_{file_name}_subsampled.txt\")\n",
    "    process_genome_into_chunks(subsample, subsample_path)\n",
    "    return subsample_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "def train_sentencepiece_tokenizer(input_file, model_prefix, vocab_size=10000):\n",
    "    \"\"\"train a sentencepiece tokenizer and save the model\"\"\"\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=input_file,\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=vocab_size,\n",
    "        model_type='bpe',\n",
    "        character_coverage=0.9995,\n",
    "        max_sentence_length=5000,\n",
    "        hard_vocab_limit=False\n",
    "    )\n",
    "    #print(f\"tokenizer and vocab saved: {model_prefix}.model, {model_prefix}.vocab\")\n",
    "\n",
    "def save_tokenized_sequence(model_prefix, genome, output_file):\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(f\"{model_prefix}.model\")\n",
    "    tokens = sp.encode_as_pieces(genome)\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\" \".join(tokens))\n",
    "    #print(f\"tokenized seq saved: {output_file}\")\n",
    "\n",
    "def process_all_genomes(base_dir):\n",
    "    \"\"\"process each genome file (both cds and ncrna files) individually for each org\"\"\"\n",
    "    for organism_dir in os.listdir(base_dir):\n",
    "        organism_path = os.path.join(base_dir, organism_dir)\n",
    "        if os.path.isdir(organism_path):\n",
    "            for file in os.listdir(organism_path):\n",
    "                file_path = os.path.join(organism_path, file)\n",
    "                if \"cds\" in file:\n",
    "                    process_single_file(file_path, \"cds\", organism_dir)\n",
    "                elif \"ncrna\" in file:\n",
    "                    process_single_file(file_path, \"ncrna\", organism_dir)\n",
    "\n",
    "def process_single_file(file_path, sequence_type, organism):\n",
    "    \"\"\"process a single genome file (either cds or ncrna)\"\"\"\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    #get the merged seq directly wo saving it separately\n",
    "    genome, sequence_count = join_cds_ncrna(file_path)\n",
    "\n",
    "    if not genome:\n",
    "        print(f\"{file_path} is empty and/or contains no valid sequences, so skipping...\")\n",
    "        return\n",
    "\n",
    "    #and then subsample if necessary\n",
    "    subsampled_sequence = subsample_genome(genome)\n",
    "    subsampled_genome_path = save_subsampled_genome(organism, file_name, subsampled_sequence)\n",
    "\n",
    "    model_prefix = f\"{tokenizers_dir}/{organism}_{file_name}_tokenizer\"\n",
    "    train_sentencepiece_tokenizer(subsampled_genome_path, model_prefix)\n",
    "\n",
    "    #debugging\n",
    "\n",
    "    if os.path.exists(subsampled_genome_path) and os.path.getsize(subsampled_genome_path) > 0:\n",
    "        #print(f\"file path: {subsampled_genome_path}, file size: {os.path.getsize(subsampled_genome_path)} bytes\")\n",
    "        with open(subsampled_genome_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            #print(f\"First 5 lines of {subsampled_genome_path}:\\n{lines[:5]}\")  \n",
    "    else:\n",
    "        #print(f\"error: {subsampled_genome_path} is empty or doesnt exist. Skipping tokenizer training\")\n",
    "        return\n",
    "\n",
    "    tokenized_output_file = f\"{subsampled_genomes_dir}/{organism}_{file_name}_tokenized.txt\"\n",
    "    save_tokenized_sequence(model_prefix, genome, tokenized_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_all_genomes(base_genomes_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
