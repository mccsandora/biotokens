{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved in /Users/tiananoll-walker/Documents/biotokens/used_code/aeropyrum_test_output.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bz2\n",
    "import lzma\n",
    "import gzip\n",
    "import csv\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "base_dir = os.path.join(script_dir, \"cds_ncrna_processing_and_tokenization/tokenized_sequences\")\n",
    "output_csv = os.path.join(script_dir, \"aeropyrum_comp_test_output.csv\")\n",
    "geco3_binary = os.path.join(script_dir, \"../geco3/src/GeCo3\") \n",
    "\n",
    "def remove_delimiters(sequence):\n",
    "    #removes anything that's not a  letter\n",
    "    return ''.join([c for c in sequence if c.isalpha()])\n",
    "\n",
    "def compute_geco3_metrics(file_path, clean_sequence, file_type):\n",
    "    \"\"\"Runs GeCo3 compression on the cleaned DNA sequence without spaces.\"\"\"\n",
    "    if file_type == \"pep\":\n",
    "        return None  #skip protein sequences, non ACGT\n",
    "    \n",
    "    try:\n",
    "        temp_clean_path = file_path + \"_cleaned.txt\"\n",
    "        with open(temp_clean_path, \"w\") as temp_file:\n",
    "            temp_file.write(clean_sequence)\n",
    "\n",
    "        compressed_file = f\"{temp_clean_path}.co\"\n",
    "        subprocess.run(\n",
    "            [geco3_binary, \"-l\", \"1\", \"-lr\", \"0.06\", \"-hs\", \"8\", temp_clean_path],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "        )\n",
    "\n",
    "        if os.path.exists(compressed_file):\n",
    "            compressed_size = os.path.getsize(compressed_file)\n",
    "            os.remove(compressed_file)\n",
    "            os.remove(temp_clean_path)  \n",
    "            return compressed_size\n",
    "        else:\n",
    "            print(f\"didn't produce a compressed file: {file_path}\")\n",
    "            os.remove(temp_clean_path)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\" error running geco3: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def compute_jsd(freq1, freq2):\n",
    "    \"\"\"computes the jensen shannon divergence (JSD) between 2 frequency distributions.\n",
    "    \n",
    "    jsd is used to measure how dif or similar 2 probability distributions are.\n",
    "    It is a **symmetric** and **smoothed** version of kullback-leibler divergence.\n",
    "    \n",
    "    my thinking: we can use jsd to compare\n",
    "    - **The observed token frequency distribution** (e.g., actual token counts in a genome)\n",
    "    - **A reference distribution** (a uniform distribution)\n",
    "    \n",
    "    **why is this relevant?**\n",
    "    - if jsd is **close to 0**, the 2 distributions are very similar (aka tokens are uniformly distributed).\n",
    "    - if it is **closer to 1**, the distributions are very diff (some tokens are much more frequent than others).\n",
    "    - this in theory should help quantify how structured and/or repetitive the tokenized sequences are. (I think?)\n",
    "    \"\"\"\n",
    "\n",
    "    #freq1 and freq2 are arrays of token frequencies. freq1 computes total num of token occurrences. division creates normalized prob dist\n",
    "    prob1 = np.array(freq1) / np.sum(freq1)\n",
    "    #later in compute_compression_metrics we set freq2 to np.ones(len(token_freq)) so that all tokens are given equal weight creating uniform dist.\n",
    "    prob2 = np.array(freq2) / np.sum(freq2)\n",
    "    #make sure its in bits so its bounded between 0 and 1 \n",
    "    return jensenshannon(prob1, prob2, base=2)\n",
    "\n",
    "def compute_compression_metrics(file_path, file_type):\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read().strip()\n",
    "        tokenized_content = content.replace('â–', '').replace('.', '').split()\n",
    "        clean_content = remove_delimiters(content)\n",
    "\n",
    "    #dict mapping each token to its freq\n",
    "    token_counts = Counter(tokenized_content)\n",
    "    \n",
    "    token_freq = np.array(list(token_counts.values()))\n",
    "\n",
    "    #total num of tokens in seq, including duplicates:\n",
    "    token_count = len(tokenized_content)\n",
    "\n",
    "    #distintc tokens in seq only:\n",
    "    unique_token_count = len(set(tokenized_content))\n",
    "\n",
    "    #total num of chars in cleaned seq with spaces and delimiters removed:\n",
    "    og_size_chars = len(clean_content)\n",
    "\n",
    "    #raw size of raw tokenized txt file before any comp\n",
    "    og_size_bytes = len(content.encode('utf-8'))\n",
    "\n",
    "    jsd_value = compute_jsd(token_freq, np.ones(len(token_freq))) if len(token_freq) > 1 else 0\n",
    "\n",
    "    gzip_size = len(gzip.compress(clean_content.encode('utf-8')))\n",
    "    bzip2_size = len(bz2.compress(clean_content.encode('utf-8')))\n",
    "    lzma_size = len(lzma.compress(clean_content.encode('utf-8')))\n",
    "\n",
    "    # token compression factor\n",
    "\n",
    "    #maps freq toks to single chars for comp, encodes the seq using reduced token rep \n",
    "    tok2chr = {tok: chr(k) for k, (tok, _) in enumerate(token_counts.most_common())}\n",
    "    encoded_sequence = ''.join(tok2chr[tok] for tok in tokenized_content if tok in tok2chr)\n",
    "    len_encoded = len(encoded_sequence)\n",
    "    len_vocab = sum(len(tok) + 1 for tok in tok2chr.keys())\n",
    "    tk_compression_ratio = (len_encoded + len_vocab) / og_size_chars\n",
    "\n",
    "    gzip_ratio = gzip_size / og_size_chars\n",
    "    bzip2_ratio = bzip2_size / og_size_chars\n",
    "    lzma_ratio = lzma_size / og_size_chars\n",
    "\n",
    "    #make sure geco only runs on og cleaned files w no spaces or delimiters\n",
    "    geco3_compressed_size = compute_geco3_metrics(file_path, clean_content,file_type)\n",
    "    geco3_ratio = (geco3_compressed_size / og_size_chars) if geco3_compressed_size else None\n",
    "\n",
    "    return {\n",
    "        \"og_size_chars\": og_size_chars,\n",
    "        \"og_size_bytes\": og_size_bytes,\n",
    "        \"token_count\": token_count,\n",
    "        \"unique_token_count\": unique_token_count,\n",
    "        \"gzip_ratio\": gzip_ratio,\n",
    "        \"bzip2_ratio\": bzip2_ratio,\n",
    "        \"lzma_ratio\": lzma_ratio,\n",
    "        \"tk_compression_ratio\": tk_compression_ratio,\n",
    "        \"geco3_ratio\": geco3_ratio,\n",
    "        \"jsd\": jsd_value,\n",
    "        \"gzip_size\": gzip_size,\n",
    "        \"bzip2_size\": bzip2_size,\n",
    "        \"lzma_size\": lzma_size,\n",
    "        \"tk_compressed_size\": tk_compression_ratio * og_size_chars,\n",
    "        \"geco3_compressed_size\": geco3_ratio * og_size_chars if geco3_ratio else None\n",
    "    }\n",
    "\n",
    "def extract_file_details(file_path):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    parts = file_name.split(\"_\")\n",
    "\n",
    "    organism = \"_\".join(parts[:-3])\n",
    "\n",
    "    if \"cds\" in file_name:\n",
    "        file_type = \"cds\"\n",
    "    elif \"ncrna\" in file_name:\n",
    "        file_type = \"ncrna\"\n",
    "    elif \"pep\" in file_name:\n",
    "        file_type = \"pep\"\n",
    "    else:\n",
    "        return None, None, None  \n",
    "\n",
    "    vocab_size = int(parts[-2].replace(\"vocab\", \"\")) if 'vocab' in parts[-2] else None\n",
    "\n",
    "    return organism, file_type, vocab_size\n",
    "\n",
    "def process_aeropyrum_files(base_dir, output_csv):\n",
    "    # runs test to process Aeropyrum_pernix files for all file types and vocab sizes\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt') and \"Aeropyrum_pernix\" in file:\n",
    "                file_path = os.path.join(root, file)\n",
    "\n",
    "                try:\n",
    "                    organism, file_type, vocab_size = extract_file_details(file_path)\n",
    "                    if file_type is None:\n",
    "                        continue\n",
    "                    \n",
    "                    metrics = compute_compression_metrics(file_path, file_type)\n",
    "\n",
    "                    results.append([\n",
    "                        organism,\n",
    "                        file_type,\n",
    "                        vocab_size,\n",
    "                        metrics[\"token_count\"],\n",
    "                        metrics[\"unique_token_count\"],\n",
    "                        metrics[\"og_size_chars\"],\n",
    "                        metrics[\"og_size_bytes\"],\n",
    "                        metrics[\"gzip_ratio\"],\n",
    "                        metrics[\"bzip2_ratio\"],\n",
    "                        metrics[\"lzma_ratio\"],\n",
    "                        metrics[\"tk_compression_ratio\"],\n",
    "                        metrics[\"geco3_ratio\"],\n",
    "                        metrics[\"jsd\"],\n",
    "                        metrics[\"gzip_size\"],\n",
    "                        (1 - metrics[\"gzip_ratio\"]) * 100,\n",
    "                        metrics[\"bzip2_size\"],\n",
    "                        (1 - metrics[\"bzip2_ratio\"]) * 100,\n",
    "                        metrics[\"lzma_size\"],\n",
    "                        (1 - metrics[\"lzma_ratio\"]) * 100,\n",
    "                        metrics[\"tk_compressed_size\"],\n",
    "                        (1 - metrics[\"tk_compression_ratio\"]) * 100,\n",
    "                        metrics[\"geco3_compressed_size\"] if metrics[\"geco3_compressed_size\"] else None,\n",
    "                        (1 - metrics[\"geco3_ratio\"]) * 100 if metrics[\"geco3_ratio\"] else None,\n",
    "                    ])\n",
    "                except Exception as e:\n",
    "                    print(f\"error processing file: {file_path}: {e}\")\n",
    "\n",
    "    with open(output_csv, mode='w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\n",
    "            \"Organism\", \"File Type\", \"Vocab Size\", \"Token Count\", \"Unique Token Count\",\n",
    "            \"Processed Seq Length\", \"File Size(Bytes)\",\n",
    "            \"GZIP Ratio\", \"BZIP2 Ratio\", \"LZMA Ratio\", \"Tk Compression Factor\", \"GeCo3 Ratio\", \"JSD\",\n",
    "            \"GZIP Ratio Compressed Size (Bytes)\", \"GZIP Ratio Compression Gain (%)\",\n",
    "            \"BZIP2 Ratio Compressed Size (Bytes)\", \"BZIP2 Ratio Compression Gain (%)\",\n",
    "            \"LZMA Ratio Compressed Size (Bytes)\", \"LZMA Ratio Compression Gain (%)\",\n",
    "            \"Tk Compression Factor Compressed Size (Bytes)\", \"Tk Compression Factor Compression Gain (%)\",\n",
    "            \"GeCo3 Ratio Compressed Size (Bytes)\", \"GeCo3 Ratio Compression Gain (%)\"\n",
    "        ])\n",
    "        writer.writerows(results)\n",
    "\n",
    "    print(f\"results in {output_csv}\")\n",
    "\n",
    "process_aeropyrum_files(base_dir, output_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
