{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import sentencepiece as spm\n",
    "import Levenshtein\n",
    "import matplotlib.pyplot as plt\n",
    "from Bio.Seq import Seq\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "from Bio.Align import PairwiseAligner\n",
    "import itertools\n",
    "import nx_parallel as nxp\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes for this notebook:\n",
    "\n",
    "- it is too big to run all the orgs at once. you have to run each org separately. Bigger genomes that aren't subsampled take a long time, even with parallelization.\n",
    "-to run this, first run the sp_and_kmer_tokenizers.ipynb notebook (https://github.com/mccsandora/biotokens/blob/main/sp_and_kmer_tokenizers.ipynb) and find the optimal vocab size for each genome. then input that result into the first code cell.\n",
    "- make sure that you run all code cells with random subgraphs multiple times to get a representative understanding of the network structures and their variations\n",
    "- Ideally I think it would be interesting to try compression methods (similar to what they did in this paper: https://arxiv.org/pdf/2401.14025) after analysis to see if compression is different across dif domains, and if/how any big differences in the network structures might affect compressibility. It would be so cool to train some ML models to predict compressibility based on analysis results and network metrics, and better understand factors that influence compression efficiency (even though I think this analysis infers a lot of that), but idk if we'll have time for that\n",
    "- Could be cool to investigate further into a method to map high degree tokens, high entropy regions and highly connected subgraphs to functional elements(I think maybe introns and exons would be easiest? Idk if that's doable but if it is can try to write code for this but I'll need a bio person's help.)\n",
    "-still working on the entropy part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "tokenizers_dir = \"/Users/tiananoll-walker/Documents/biotokens/tokenizers\"\n",
    "\n",
    "def load_vocab(org, vocab_size, method):\n",
    "    vocab_path = os.path.join(tokenizers_dir, f\"{org}_{vocab_size}_{method}.vocab\")\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        vocab = [line.split('\\t')[0] for line in f.readlines()[3:]] \n",
    "    return vocab\n",
    "\n",
    "#org = \"Theileria orientalis strain Shintoku\"\n",
    "#org =  \"Nanobdella aerobiophila\"\n",
    "org =  \"Candidatus Karelsulcia muelleri\"\n",
    "#org =  \"Malassezia restricta\"\n",
    "#org =  \"Caenorhabditis elegans\"\n",
    "#org = \"Ostreococcus lucimarinus CCE9901\"\n",
    "#org =   \"Rice yellow mottle virus satellite\"\n",
    "vocab_size = 6000\n",
    "method =  'kmer' #or \"sp\"  \n",
    "\n",
    "tokens = load_vocab(org, vocab_size, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"the levenshtein distance is a metric for measuring the difference between 2 sequences. it counts the minimum number of single character edits required to change one word into the other.\n",
    "in our analysis, we use the lev distance to determine how closely related different tokens are. by constructing a graph where nodes are tokens and edges exist between\n",
    "nodes that are levenshtein distance 1 apart, we can visualize and analyze the relationship between tokens\"\"\"\n",
    "\n",
    "def is_distance_one(seq1, seq2):\n",
    "    if abs(len(seq1) - len(seq2)) > 1:\n",
    "        return False\n",
    "    overlap1 = {seq1[:i] + seq1[i+1:] for i in range(len(seq1))}\n",
    "    overlap2 = {seq2[:i] + seq2[i+1:] for i in range(len(seq2))}\n",
    "    return not overlap1.isdisjoint(overlap2)\n",
    "\n",
    "def process_token_pairs(tokens, idx):\n",
    "    token1 = tokens[idx]\n",
    "    edges = []\n",
    "    for j in range(idx + 1, len(tokens)):\n",
    "        token2 = tokens[j]\n",
    "        if is_distance_one(token1, token2):\n",
    "            edges.append((token1, token2))\n",
    "    return edges\n",
    "\n",
    "def build_lev_graph(tokens):\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(tokens)\n",
    "    \n",
    "    results = Parallel(n_jobs=-2)(delayed(process_token_pairs)(tokens, i) for i in range(len(tokens)))\n",
    "\n",
    "    for edge_list in results:\n",
    "        G.add_edges_from(edge_list)\n",
    "    return G\n",
    "\n",
    "#tokens=load_vocab(\"Nanobdella aerobiophila\", 6000, method) \n",
    "#tokens = load_vocab(\"e_coli_genome\", 37000)\n",
    "#tokens=load_vocab(\"Caenorhabditis_elegans\", 31000)\n",
    "tokens = load_vocab(\"Candidatus Karelsulcia muelleri\", 6000, method)\n",
    "levenshtein_graph = build_lev_graph(tokens)\n",
    "print(f\"levenshtein graph has {len(levenshtein_graph.nodes())} nodes and {len(levenshtein_graph.edges())} edges\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 minutes for ecoli. not necessary because its so large but interesting to look at \n",
    "#visualizing subgraph that only has nodes w at least 5 edges connected to it \n",
    "def visualize_graph(G, threshold=5):\n",
    "    filtered_nodes = [node for node, degree in G.degree() if degree >= threshold]\n",
    "    if not filtered_nodes:\n",
    "        print(\"no nodes meet threshold \")\n",
    "        return\n",
    "    \n",
    "    G_filtered = G.subgraph(filtered_nodes)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    pos = nx.spring_layout(G_filtered, seed=42)\n",
    "    nx.draw(G_filtered, pos, with_labels=True, node_size=100, font_size=8, node_color='pink', font_color='black', edge_color='gray')\n",
    "    plt.title(\"levenshtein graph\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_graph(levenshtein_graph, threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"degree distribution shows how many connections (edges) each node has to other nodes\"\"\"\n",
    "#degree : num of edges connected to a node\n",
    "#frequency : number of nodes w that degree\n",
    "\n",
    "def plot_degree_distribution(G):\n",
    "    degrees = [G.degree(n) for n in G.nodes()]\n",
    "    plt.figure()\n",
    "    plt.hist(degrees, bins=range(1, max(degrees)+1))\n",
    "    plt.title(\"degree distribution\")\n",
    "    plt.xlabel(\"degree\")\n",
    "    plt.ylabel(\"frequency\")\n",
    "    plt.show()\n",
    "\n",
    "plot_degree_distribution(levenshtein_graph)\n",
    "\n",
    "avg_degree = np.mean([deg for node, deg in levenshtein_graph.degree()])\n",
    "print(f\"avg degree: {avg_degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measures the similarity of connections in the graph w respect to the degree\n",
    "#between -1 and 1, high val means that nodes w high degrees more likely to connect w other high degree nodes\n",
    "\n",
    "ac = nx.degree_assortativity_coefficient(levenshtein_graph)\n",
    "print(f\"degree assortativity coefficient : {ac}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#visualizing nodes that have a degree >= to the avg degree +1 SD, highlighting the more connected nodes in the grap\n",
    "\n",
    "avg_degree = np.mean([degree for _, degree in levenshtein_graph.degree()])\n",
    "std_dev_degree = np.std([degree for _, degree in levenshtein_graph.degree()])\n",
    "\n",
    "degree_threshold = avg_degree + std_dev_degree\n",
    "\n",
    "def visualize_high_degree_nodes(G, degree_threshold):\n",
    "    high_degree_nodes = [node for node, degree in G.degree() if degree >= degree_threshold]\n",
    "    if not high_degree_nodes:\n",
    "        print(\"no nodes meet threshold \")\n",
    "        return\n",
    "    \n",
    "    G_high = G.subgraph(high_degree_nodes)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    pos = nx.spring_layout(G_high, seed=42)\n",
    "    nx.draw(G_high, pos, with_labels=True, node_size=100, font_size=8, node_color='pink', font_color='black', edge_color='gray')\n",
    "    plt.title(\"high degree nodes graph\")\n",
    "    plt.show()\n",
    "\n",
    "high_d_nodes = visualize_high_degree_nodes(levenshtein_graph, degree_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"centrality measures help identify the most important nodes in a network. We can use these measures to analyze the role/importance of each token in the fitness landscape.\"\"\"\n",
    "#12 mins to run for nano\n",
    "#120 mins for ecoli\n",
    "\n",
    "def compute_centrality_measures(G, max_iter=1000, tol=1e-06):\n",
    "    centrality_measures = {\n",
    "        \"degree_centrality\": nx.degree_centrality(G),\n",
    "        \"closeness_centrality\": nx.closeness_centrality(G),\n",
    "        \"betweenness_centrality\": nxp.betweenness_centrality(G),\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        centrality_measures[\"eigenvector_centrality\"] = nx.eigenvector_centrality(G, max_iter=max_iter, tol=tol)\n",
    "    except nx.PowerIterationFailedConvergence as e:\n",
    "        print(f\"eig calculation failed to converge: {e}\")\n",
    "    \n",
    "    return centrality_measures\n",
    "\n",
    "centrality_measures = compute_centrality_measures(levenshtein_graph)\n",
    "#for measure, values in centrality_measures.items():\n",
    "#    print(f\"{measure}:\")\n",
    "#    for node, value in values.items():\n",
    "#        print(f\"{node}: {value:.4f}\")\n",
    "#    print()\n",
    "\n",
    "def plot_centrality_distribution(centrality_dict, title):\n",
    "    values = list(centrality_dict.values())\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(values, bins=30, color='purple', edgecolor='black')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('centrality val')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.show()\n",
    "\n",
    "plot_centrality_distribution(centrality_measures[\"degree_centrality\"], \"degree centrality dist\")\n",
    "plot_centrality_distribution(centrality_measures[\"closeness_centrality\"], \"closeness centrality dist\")\n",
    "plot_centrality_distribution(centrality_measures[\"betweenness_centrality\"], \"betweenness centrality dist\")\n",
    "plot_centrality_distribution(centrality_measures[\"eigenvector_centrality\"], \"eigenvector centrality dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11 mins for ecoli\n",
    "\n",
    "def detect_communities(G):\n",
    "    communities = list(nx.algorithms.community.greedy_modularity_communities(G))\n",
    "    return communities\n",
    "\n",
    "communities = detect_communities(levenshtein_graph)\n",
    "print(f\"found {len(communities)} communities \")\n",
    "#for i, community in enumerate(communities):\n",
    "#    print(f\"community {i + 1}: {sorted(community)}\")\n",
    "\n",
    "\n",
    "community_sizes = [len(c) for c in communities]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(community_sizes, bins=50, color='blue', edgecolor='black')\n",
    "plt.title(\"community size distribution\")\n",
    "plt.xlabel(\"community size\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"mean com size: {np.mean(community_sizes)}\")\n",
    "print(f\"median com size: {np.median(community_sizes)}\")\n",
    "print(f\"standard dev of com sizes: {np.std(community_sizes)}\")\n",
    "print(f\"biggest com size: {np.max(community_sizes)}\")\n",
    "print(f\"smallest com size: {np.min(community_sizes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"path lengths measure the shortest num of edges to travel between nodes in the graph. so distance of 1 is a direct neighbor, 2 is two doors down, more than 2 means there are multiple intermediary tokens required to connect them  \"\"\"\n",
    "#40 mins for ecoli\n",
    "\n",
    "#diameter: longest shortest path between any two nodes\n",
    "#avg path length: any two nodes in the graph are about ~avg path length~ apart\n",
    "\n",
    "def compute_path_lengths(G):\n",
    "    path_lengths = dict(nxp.all_pairs_shortest_path_length(G))\n",
    "    return path_lengths\n",
    "\n",
    "path_lengths = compute_path_lengths(levenshtein_graph)\n",
    "print(f\"computed path lengths for {len(path_lengths)} nodes \")\n",
    "#for node, lengths in path_lengths.items():\n",
    "#    print(f\"{node}: {lengths}\")\n",
    "\n",
    "\n",
    "all_path_lengths = [length for lengths in path_lengths.values() for length in lengths.values()]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(all_path_lengths, bins=50, kde=False, color='blue')\n",
    "plt.title('path length distribution')\n",
    "plt.xlabel('path length')\n",
    "plt.ylabel('frequency')\n",
    "plt.show()\n",
    "\n",
    "if nx.is_connected(levenshtein_graph):\n",
    "    avg_p_len = nx.average_shortest_path_length(levenshtein_graph)\n",
    "    diameter = nx.diameter(levenshtein_graph)\n",
    "else:\n",
    "    largest_cc = max(nx.connected_components(levenshtein_graph), key=len)\n",
    "    subgraph = levenshtein_graph.subgraph(largest_cc)\n",
    "    avg_p_len = nx.average_shortest_path_length(subgraph)\n",
    "    diameter = nx.diameter(subgraph)\n",
    "\n",
    "print(f\"avg path length: {avg_p_len}\")\n",
    "print(f\"diameter: {diameter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"the clustering coefficient measures the degree to which nodes in a graph tend to cluster together.\n",
    "It's calculated for each node and represents the ratio of the num of actual edges between the node’s \n",
    "neighbors to the number of possible edges between the node’s neighbors. A high clustering coefficient \n",
    "indicates that a node’s neighbors are also closely connected to each other.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def compute_clustering_coefficient(G):\n",
    "    clustering_coefficient = nx.clustering(G)\n",
    "    return clustering_coefficient\n",
    "\n",
    "clustering_coefficient = compute_clustering_coefficient(levenshtein_graph)\n",
    "#print(\"clustering coefficients:\")\n",
    "#for node, coefficient in clustering_coefficient.items():\n",
    "#    print(f\"{node}: {coefficient:.4f}\")\n",
    "\n",
    "def plot_clustering_coefficient_distribution(clustering_coefficient):\n",
    "    coefficients = list(clustering_coefficient.values())\n",
    "    plt.figure()\n",
    "    plt.hist(coefficients, bins=20, color='pink', edgecolor='black')\n",
    "    plt.title(\"clustering coefficient distribution\")\n",
    "    plt.xlabel(\"clustering coefficient\")\n",
    "    plt.ylabel(\"frequency\")\n",
    "    plt.show()\n",
    "\n",
    "plot_clustering_coefficient_distribution(clustering_coefficient)\n",
    "\n",
    "avg_cc = nx.average_clustering(levenshtein_graph)\n",
    "print(f\"avg cc: {avg_cc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ratio of num of edges to num of possible edges. measures how interconnected the graph is\"\"\"\n",
    "\n",
    "density = nx.density(levenshtein_graph)\n",
    "print(f\"graph density: {density}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random sample\n",
    "\n",
    "def random_sub(G, size):\n",
    "    nodes = random.sample(list(G.nodes()), size)\n",
    "    subgraph = G.subgraph(nodes)\n",
    "    return subgraph\n",
    "\n",
    "random_subgraph = random_sub(levenshtein_graph, 100)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(random_subgraph)\n",
    "nx.draw(random_subgraph, pos, with_labels=True, node_size=500,\n",
    "        font_size=10, node_color='pink', font_color='black', edge_color='gray')\n",
    "plt.title('random subgraph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample via breadth first search. it's random but biased toward connectivity\n",
    "\n",
    "def connected_random_subgraph(G, size):\n",
    "    start_node = random.choice(list(G.nodes()))\n",
    "    visited = set([start_node])\n",
    "    queue = [start_node]\n",
    "    \n",
    "    while queue and len(visited) < size:\n",
    "        node = queue.pop(0)\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        random.shuffle(neighbors)\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor not in visited:\n",
    "                visited.add(neighbor)\n",
    "                queue.append(neighbor)\n",
    "                if len(visited) == size:\n",
    "                    break\n",
    "    \n",
    "    return G.subgraph(visited)\n",
    "\n",
    "random_subgraph = connected_random_subgraph(levenshtein_graph, 100)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(random_subgraph)\n",
    "nx.draw(random_subgraph, pos, with_labels=True, node_size=500,\n",
    "        font_size=8, node_color='pink', font_color='black', edge_color='gray')\n",
    "plt.title('random connected subgraph via bfs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ego graph centered around a specific randomly selected node w radius of 2(the ego node, all nodes directly connected to it, and all nodes connected to those nodes)\n",
    "#run a few times\n",
    "\n",
    "def ego_sub(G, node, radius):\n",
    "    subgraph = nx.ego_graph(G, node, radius=radius)\n",
    "    return subgraph\n",
    "\n",
    "ego_node = random.choice(list(levenshtein_graph.nodes))\n",
    "ego_subgraph = ego_sub(levenshtein_graph, ego_node, radius=2)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(ego_subgraph)\n",
    "nx.draw(ego_subgraph, pos, with_labels=True, node_size=100, font_size=8, node_color='lightgreen', font_color='black', edge_color='gray')\n",
    "plt.title(f\"ego graph centered around token {ego_node}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"the smith-waterman algo is used for local sequence alignment. it finds the optimal local alignment between two sequences by identifying regions of high similarity.\n",
    "unlike the needleman-wunsch algo, which performs a global alignment, the smith-waterman algorithm focuses on the most similar parts of the sequences, allowing for gaps.\n",
    "essentially we measure the local similarity between different tokens by computing alignment scores. red regions indicate higher local sequence similarity between those token pairs\"\"\"\n",
    "\n",
    "subsampled_tokens = random.sample(tokens, 100)\n",
    "\n",
    "def calculate_sw_score(token1, token2):\n",
    "    aligner = PairwiseAligner()\n",
    "    aligner.mode = 'local'\n",
    "    aligner.match_score = 2\n",
    "    aligner.mismatch_score = -1\n",
    "    aligner.open_gap_score = -2\n",
    "    aligner.extend_gap_score = -1\n",
    "    alignments = aligner.align(token1, token2)\n",
    "    return (token1, token2, alignments.score)\n",
    "\n",
    "def process_pairs(tokens):\n",
    "    pairs = [(tokens[i], tokens[j]) for i in range(len(tokens)) for j in range(i + 1, len(tokens))]\n",
    "    \n",
    "    results = Parallel(n_jobs=-2)(delayed(calculate_sw_score)(pair[0], pair[1]) for pair in tqdm(pairs, total=len(pairs)))\n",
    "    return results\n",
    "\n",
    "results = process_pairs(subsampled_tokens)\n",
    "\n",
    "df = pd.DataFrame(results, columns=[\"token1\", \"token2\", \"score\"])\n",
    "\n",
    "pivot_table = df.pivot_table(index=\"token1\", columns=\"token2\", values=\"score\", fill_value=0)\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(pivot_table, annot=False, cmap=\"coolwarm\", cbar=True, linewidths=.5, linecolor='gray')\n",
    "plt.title(\"smith-waterman score heatmap\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"the needleman wunsch algo is another algo for sequence alignment. its used to align protein or nucleotide sequences by finding the optimal match between them, allowing for gaps. \n",
    "the algo works by creating a score matrix and finding the highest scoring alignment. in our fitness landscape analysis we use\n",
    "the NW algo to measure the similarity between dif tokens by computing alignment scores. \"\"\"\n",
    "\n",
    "subsampled_tokens = random.sample(tokens, 100)\n",
    "\n",
    "def needleman_wunsch(seq1, seq2, match=1, mismatch=-1, gap=-1):\n",
    "    n = len(seq1)\n",
    "    m = len(seq2)\n",
    "    score = np.zeros((n + 1, m + 1))\n",
    "    \n",
    "    for i in range(1, n + 1):\n",
    "        score[i][0] = score[i - 1][0] + gap\n",
    "    for j in range(1, m + 1):\n",
    "        score[0][j] = score[0][j] + gap\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            match_mismatch = match if seq1[i - 1] == seq2[j - 1] else mismatch\n",
    "            score[i][j] = max(\n",
    "                score[i - 1][j - 1] + match_mismatch,\n",
    "                score[i - 1][j] + gap,\n",
    "                score[i][j - 1] + gap\n",
    "            )\n",
    "    return score[n][m]\n",
    "\n",
    "def compute_nw_score(pair):\n",
    "    seq1, seq2 = pair\n",
    "    return (seq1, seq2, needleman_wunsch(seq1, seq2))\n",
    "\n",
    "def process_pairs(tokens):\n",
    "    pairs = [(tokens[i], tokens[j]) for i in range(len(tokens)) for j in range(i + 1, len(tokens))]\n",
    "    results = Parallel(n_jobs=-2)(delayed(compute_nw_score)(pair) for pair in tqdm(pairs))\n",
    "    return results\n",
    "\n",
    "nw_scores = process_pairs(subsampled_tokens)\n",
    "\n",
    "#for seq1, seq2, score in nw_scores:\n",
    "#    print(f\"NW score between {seq1} and {seq2}: {score}\")\n",
    "\n",
    "df = pd.DataFrame(nw_scores, columns=[\"token1\", \"token2\", \"score\"])\n",
    "pivot_table = df.pivot_table(index=\"token1\", columns=\"token2\", values=\"score\", fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(pivot_table, annot=False, cmap=\"coolwarm\", cbar=True, linewidths=.5, linecolor='gray')\n",
    "plt.title(\"needleman-wunsch score heatmap\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shannon_entropy(tokens):\n",
    "    token_counts = Counter(tokens)\n",
    "    total_count = len(tokens)\n",
    "    entropy = -sum((count / total_count) * math.log2(count / total_count) for count in token_counts.values())\n",
    "    return entropy\n",
    "\n",
    "genome_entropy = shannon_entropy(tokens)\n",
    "print(f\"shannon entropy for entire genome: {genome_entropy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I don't think this code is right lol, so I'm commenting it out until further notice. \n",
    "\n",
    "window_size = 1000\n",
    "step_size = 500\n",
    "entropies = []\n",
    "positions = []\n",
    "\n",
    "for i in range(0, len(tokens) - window_size + 1, step_size):\n",
    "    window_tokens = tokens[i:i + window_size]\n",
    "    window_entropy = shannon_entropy(window_tokens)\n",
    "    entropies.append(window_entropy)\n",
    "    positions.append(i)\n",
    "\n",
    "high_entropy_threshold = np.percentile(entropies, 95)\n",
    "print(f\"high entropy threshold (95th percentile): {high_entropy_threshold}\")\n",
    "\n",
    "high_entropy_regions = [(positions[i], positions[i] + window_size) for i, entropy in enumerate(entropies) if entropy >= high_entropy_threshold]\n",
    "print(f\"num of high entropy regions: {len(high_entropy_regions)}\")\n",
    "print(f\"high entropy regions: {high_entropy_regions}\")\n",
    "\n",
    "def merge_regions(regions):\n",
    "    if not regions:\n",
    "        return []\n",
    "\n",
    "    regions = sorted(regions, key=lambda x: x[0])\n",
    "    merged_regions = [regions[0]]\n",
    "    \n",
    "    for current in regions:\n",
    "        last = merged_regions[-1]\n",
    "        if current[0] <= last[1]:\n",
    "            merged_regions[-1] = (last[0], max(last[1], current[1]))\n",
    "        else:\n",
    "            merged_regions.append(current)\n",
    "    \n",
    "    return merged_regions\n",
    "\n",
    "merged_high_entropy_regions = merge_regions(high_entropy_regions)\n",
    "#print(f\"merged high entropy regions: {merged_high_entropy_regions}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(positions, entropies, marker='o', linestyle='-', label='Entropy')\n",
    "\n",
    "for start, end in merged_high_entropy_regions:\n",
    "    plt.axvspan(start, end, color='red', alpha=0.3, label='High Entropy Region' if start == merged_high_entropy_regions[0][0] else \"\")\n",
    "\n",
    "plt.title(\"shannon entropy across the tokens\")\n",
    "plt.xlabel(\"position in token list\")\n",
    "plt.ylabel(\"shannon entropy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.4",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
