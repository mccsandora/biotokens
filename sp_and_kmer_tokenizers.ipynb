{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "genome_sequences_dir = \"genome_sequences\"\n",
    "tokenizers_dir = \"tokenizers\"\n",
    "subsampled_genomes_dir = \"subsampled_genomes\"\n",
    "chunk_size = 4192\n",
    "default_subsample_size = 10**6\n",
    "\n",
    "os.makedirs(subsampled_genomes_dir, exist_ok=True)\n",
    "os.makedirs(tokenizers_dir, exist_ok=True)\n",
    "\n",
    "#org =  \"Candidatus Karelsulcia muelleri\",\n",
    "#org =  \"Malassezia restricta\",\n",
    "#org =  \"Caenorhabditis elegans\",\n",
    "#org = \"Ostreococcus lucimarinus CCE9901\",\n",
    "#org =  \"Rice yellow mottle virus satellite\",\n",
    "#org = \"Theileria orientalis strain Shintoku\"\n",
    "org = \"Nanobdella aerobiophila\"\n",
    "genome_filename = f\"{org.replace(' ', '_')}_cleaned.txt\"\n",
    "\n",
    "def load_genome_sequence(filename):\n",
    "    path = os.path.join(genome_sequences_dir, filename)\n",
    "    with open(path, 'r') as f:\n",
    "        genome = f.read().upper().replace('\\n', '')\n",
    "    return genome\n",
    "\n",
    "genome = load_genome_sequence(genome_filename)\n",
    "print(f\"genome size: {len(genome)}\")\n",
    "\n",
    "def process_genome_into_chunks(genome, path, chunk_size=chunk_size):\n",
    "    chunks = [genome[i:i+chunk_size] + '.' for i in range(0, len(genome), chunk_size)]\n",
    "    with open(path, 'w') as f:\n",
    "        for chunk in chunks:\n",
    "            f.write(chunk + '\\n')\n",
    "    print(f\"processed genome saved to {path}\")\n",
    "\n",
    "processed_genome_path = os.path.join(subsampled_genomes_dir, f\"{org.replace(' ', '_')}_processed.txt\")\n",
    "process_genome_into_chunks(genome, processed_genome_path)\n",
    "\n",
    "def subsample_genome(genome, size=default_subsample_size):\n",
    "    if len(genome) <= size:\n",
    "        return genome\n",
    "    elif len(genome) <= 2 * size:\n",
    "        return genome\n",
    "    return genome[:size] + genome[-size:]\n",
    "\n",
    "def save_subsampled_genome(org, subsample):\n",
    "    subsample_path = os.path.join(subsampled_genomes_dir, f\"{org.replace(' ', '_')}_subsampled.txt\")\n",
    "    process_genome_into_chunks(subsample, subsample_path)\n",
    "    return subsample_path\n",
    "\n",
    "subsampled_sequence = subsample_genome(genome)\n",
    "subsampled_genome_path = save_subsampled_genome(org, subsampled_sequence)\n",
    "\n",
    "def check_processed_file(path):\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        print(f\"{path} contains {len(lines)} lines.\")\n",
    "\n",
    "check_processed_file(processed_genome_path)\n",
    "\n",
    "def get_sequence_length(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return len(f.read().replace('\\n', ''))\n",
    "\n",
    "processed_length = get_sequence_length(processed_genome_path)\n",
    "subsampled_length = get_sequence_length(subsampled_genome_path)\n",
    "\n",
    "print(f\" processed genome length: {processed_length} bases\")\n",
    "print(f\" subsampled genome length: {subsampled_length} bases\")\n",
    "\n",
    "training_genome_path = subsampled_genome_path if subsampled_length < processed_length else processed_genome_path\n",
    "print(f\"Using {'subsampled' if subsampled_length < processed_length else 'processed'} genome for training.\")\n",
    "\n",
    "def train_sentencepiece_tokenizer_bpe(input_file, model_prefix, vocab_size, method=\"sp\"):\n",
    "    model_prefix = f\"{model_prefix}_{method}\"  # Ensure the model and vocab files are named with the \"sp\" prefix\n",
    "    model_file = model_prefix + \".model\"\n",
    "    vocab_file = model_prefix + \".vocab\"\n",
    "    \n",
    "    # Check if the model and vocab files already exist\n",
    "    if os.path.exists(model_file) and os.path.exists(vocab_file):\n",
    "        print(f\"Model and vocab files already exist: {model_file}, {vocab_file}\")\n",
    "        return\n",
    "    try:\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=input_file,\n",
    "            model_prefix=model_prefix,\n",
    "            vocab_size=vocab_size,\n",
    "            model_type='bpe',\n",
    "            character_coverage=0.9995,\n",
    "            max_sentence_length=5000,\n",
    "            hard_vocab_limit=False\n",
    "        )\n",
    "        if os.path.exists(model_file) and os.path.exists(vocab_file):\n",
    "            print(f\"model and vocab files created: {model_file}, {vocab_file}\")\n",
    "        else:\n",
    "            print(f\"error, model or vocab file not created for {model_prefix}\")\n",
    "    except Exception as e:\n",
    "        print(f\"error occurred during training: {e}\")\n",
    "        raise e\n",
    "\n",
    "def load_tokenizer_vocab(model_prefix):\n",
    "    vocab_file = model_prefix + \".vocab\"\n",
    "    vocab = pd.read_csv(vocab_file, sep='\\t', header=None)\n",
    "    vocab = vocab[0].tolist()\n",
    "    vocab = [v for v in vocab if v and re.match(r'^[ACGT]+$', v)]\n",
    "    return vocab\n",
    "\n",
    "def calculate_compression_factor(genome, tokens, vocab):\n",
    "    total_length = len(genome)\n",
    "    encoded_length = len(tokens)\n",
    "    vocab_length = sum(len(token) for token in vocab)\n",
    "    compression_factor = (encoded_length + vocab_length) / total_length\n",
    "    return compression_factor\n",
    "\n",
    "def evaluate_vocab_size(genome, processed_genome_path, vocab_size, method=\"sp\"):\n",
    "    model_prefix = os.path.join(tokenizers_dir, f\"{org}_{vocab_size}\")\n",
    "    print(f\"Training with vocab size: {vocab_size}, model prefix: {model_prefix}_{method}\")\n",
    "    train_sentencepiece_tokenizer_bpe(processed_genome_path, model_prefix, vocab_size, method)\n",
    "    \n",
    "\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(model_prefix + \".model\")\n",
    "    tokens = sp.encode_as_pieces(genome)\n",
    "    \n",
    "    vocab = load_tokenizer_vocab(model_prefix)\n",
    "    print(f\"Sample vocab for vocab size {vocab_size}: {vocab[:10]}\")\n",
    "    compression_factor = calculate_compression_factor(genome, tokens, vocab)\n",
    "    print(f'Vocab size: {vocab_size}, compression factor: {compression_factor}, number of tokens: {len(tokens)}')\n",
    "    return (vocab_size, compression_factor, len(tokens))\n",
    "\n",
    "def extended_search_vocab_size(genome, processed_genome_path, initial_vocab_size=1000, step_size=5000, max_vocab_size=80000, threshold=1):\n",
    "    genome_length = len(genome)\n",
    "    if genome_length > 10**6:\n",
    "        step_size = 10000\n",
    "        max_vocab_size = 100000\n",
    "\n",
    "    vocab_size = initial_vocab_size\n",
    "    compression_factors = []\n",
    "    optimal_vocab_size = None\n",
    "\n",
    "    while vocab_size <= max_vocab_size:\n",
    "        result = evaluate_vocab_size(genome, processed_genome_path, vocab_size)\n",
    "        if result:\n",
    "            vocab_size, compression_factor, num_tokens = result\n",
    "            compression_factors.append(result)\n",
    "            if len(compression_factors) > 1 and abs(compression_factors[-1][1] - compression_factors[-2][1]) < threshold:\n",
    "                optimal_vocab_size = vocab_size\n",
    "                break\n",
    "        vocab_size += step_size\n",
    "\n",
    "    if optimal_vocab_size is None:\n",
    "        min_compression = min(compression_factors, key=lambda x: x[1])\n",
    "        optimal_vocab_size = min_compression[0]\n",
    "\n",
    "    return compression_factors, optimal_vocab_size\n",
    "\n",
    "compression_factors, optimal_vocab_size = extended_search_vocab_size(\n",
    "    genome, processed_genome_path, initial_vocab_size=1000, step_size=5000, max_vocab_size=80000, threshold=1)\n",
    "\n",
    "if optimal_vocab_size is None:\n",
    "    print(f\"No optimal vocab size found. Compression factors: {compression_factors}\")\n",
    "else:\n",
    "    print(f\"Optimal vocab size for {org}: {optimal_vocab_size}\")\n",
    "\n",
    "def load_tokenizer(org, vocab_size, method=\"sp\"):\n",
    "    model_path = os.path.join(tokenizers_dir, f\"{org}_{vocab_size}_{method}.model\")\n",
    "    sp = spm.SentencePieceProcessor(model_file=model_path)\n",
    "    return sp\n",
    "\n",
    "\n",
    "def plot_results(compression_factors):\n",
    "    vocab_sizes, compression_values, num_tokens = zip(*compression_factors)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(vocab_sizes, compression_values)\n",
    "    plt.xlabel('vocab size')\n",
    "    plt.ylabel('compression factor')\n",
    "    plt.title('compression factor vs vocab size')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(num_tokens, compression_values)\n",
    "    plt.xlabel('number of tokens')\n",
    "    plt.ylabel('compression factor')\n",
    "    plt.title('compression factor vs number of tokens')\n",
    "    plt.grid(True)\n",
    "    plt.xlim(left=0)\n",
    "    plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x):,}'))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_tokens(tokens):\n",
    "    tokcounts = Counter(tokens).most_common()\n",
    "    print(tokcounts[:30])\n",
    "\n",
    "    plt.hist([len(t[0]) for t in tokcounts],\n",
    "            bins=np.arange(0,max([len(t[0]) for t in tokcounts])+1))\n",
    "    plt.xlabel('token length')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.title('token length distribution')\n",
    "    plt.show()\n",
    "\n",
    "    TC = [t[1] for t in tokcounts]\n",
    "    plt.plot(np.arange(0, len(TC), 1), TC)\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('rank')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_results(compression_factors)\n",
    "\n",
    "sp_model = load_tokenizer(org, optimal_vocab_size, method=\"sp\")\n",
    "sp_tokens = sp_model.encode_as_pieces(genome)\n",
    "analyze_tokens(sp_tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "genome_sequences_dir = \"genome_sequences\"\n",
    "tokenizers_dir = \"tokenizers\"\n",
    "subsampled_genomes_dir = \"subsampled_genomes\"\n",
    "chunk_size = 4192\n",
    "default_subsample_size = 10**6\n",
    "kmer_size = 6 \n",
    "\n",
    "os.makedirs(subsampled_genomes_dir, exist_ok=True)\n",
    "os.makedirs(tokenizers_dir, exist_ok=True)\n",
    "\n",
    "#org = \"Theileria orientalis strain Shintoku\"\n",
    "org =  \"Nanobdella aerobiophila\"\n",
    "#org =  \"Candidatus Karelsulcia muelleri\"\n",
    "#org =  \"Malassezia restricta\"\n",
    "#org =  \"Caenorhabditis elegans\"\n",
    "#org = \"Ostreococcus lucimarinus CCE9901\"\n",
    "#org =   \"Rice yellow mottle virus satellite\"\n",
    "\n",
    "genome_filename = f\"{org.replace(' ', '_')}_cleaned.txt\"\n",
    "\n",
    "def load_genome_sequence(filename):\n",
    "    path = os.path.join(genome_sequences_dir, filename)\n",
    "    with open(path, 'r') as f:\n",
    "        genome = f.read().upper().replace('\\n', '')\n",
    "    return genome\n",
    "\n",
    "genome = load_genome_sequence(genome_filename)\n",
    "print(f\"genome size: {len(genome)}\")\n",
    "\n",
    "def process_genome_into_chunks(genome, path, chunk_size=chunk_size):\n",
    "    chunks = [genome[i:i+chunk_size] + '.' for i in range(0, len(genome), chunk_size)]\n",
    "    with open(path, 'w') as f:\n",
    "        for chunk in chunks:\n",
    "            f.write(chunk + '\\n')\n",
    "    print(f\"processed genome saved to {path}\")\n",
    "\n",
    "processed_genome_path = os.path.join(subsampled_genomes_dir, f\"{org.replace(' ', '_')}_processed.txt\")\n",
    "process_genome_into_chunks(genome, processed_genome_path)\n",
    "\n",
    "def subsample_genome(genome, size=default_subsample_size):\n",
    "    if len(genome) <= size:\n",
    "        return genome\n",
    "    elif len(genome) <= 2 * size:\n",
    "        return genome\n",
    "    return genome[:size] + genome[-size:]\n",
    "\n",
    "def save_subsampled_genome(org, subsample):\n",
    "    subsample_path = os.path.join(subsampled_genomes_dir, f\"{org.replace(' ', '_')}_subsampled.txt\")\n",
    "    process_genome_into_chunks(subsample, subsample_path)\n",
    "    return subsample_path\n",
    "\n",
    "subsampled_sequence = subsample_genome(genome)\n",
    "subsampled_genome_path = save_subsampled_genome(org, subsampled_sequence)\n",
    "\n",
    "def check_processed_file(path):\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        print(f\"{path} contains {len(lines)} lines.\")\n",
    "\n",
    "check_processed_file(processed_genome_path)\n",
    "\n",
    "def get_sequence_length(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return len(f.read().replace('\\n', ''))\n",
    "\n",
    "processed_length = get_sequence_length(processed_genome_path)\n",
    "subsampled_length = get_sequence_length(subsampled_genome_path)\n",
    "\n",
    "print(f\" processed genome length: {processed_length} bases\")\n",
    "print(f\" subsampled genome length: {subsampled_length} bases\")\n",
    "\n",
    "training_genome_path = subsampled_genome_path if subsampled_length < processed_length else processed_genome_path\n",
    "print(f\"Using {'subsampled' if subsampled_length < processed_length else 'processed'} genome for training.\")\n",
    "\n",
    "def generate_kmers(sequence, k):\n",
    "    return [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
    "\n",
    "def save_kmers_to_file(kmers, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for kmer in kmers:\n",
    "            f.write(kmer + '\\n')\n",
    "    print(f\"K-mers saved to {file_path}\")\n",
    "\n",
    "def train_sentencepiece_tokenizer_kmer(input_file, model_prefix, vocab_size, method=\"kmer\"):\n",
    "    model_prefix = f\"{model_prefix}_{method}\" if method not in model_prefix else model_prefix\n",
    "    model_file = model_prefix + \".model\"\n",
    "    vocab_file = model_prefix + \".vocab\"\n",
    "\n",
    "    if os.path.exists(model_file) and os.path.exists(vocab_file):\n",
    "        print(f\"Model and vocab files already exist: {model_file}, {vocab_file}\")\n",
    "        return model_prefix  \n",
    "    try:\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=input_file,\n",
    "            model_prefix=model_prefix,\n",
    "            vocab_size=vocab_size,\n",
    "            model_type='bpe',\n",
    "            character_coverage=0.9995,\n",
    "            max_sentence_length=5000,\n",
    "            hard_vocab_limit=False\n",
    "        )\n",
    "        if os.path.exists(model_file) and os.path.exists(vocab_file):\n",
    "            print(f\"Model and vocab files created: {model_file}, {vocab_file}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Error: model or vocab file not created for {model_prefix}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during training: {e}\")\n",
    "        raise e\n",
    "    return model_prefix\n",
    "\n",
    "def load_tokenizer_vocab(model_prefix):\n",
    "    vocab_file = model_prefix + \".vocab\"\n",
    "    vocab = pd.read_csv(vocab_file, sep='\\t', header=None)\n",
    "    vocab = vocab[0].tolist()\n",
    "    vocab = [v for v in vocab if v and re.match(r'^[ACGT]+$', v)] \n",
    "    return vocab\n",
    "\n",
    "def calculate_compression_factor(genome, tokens, vocab):\n",
    "    total_length = len(genome)\n",
    "    encoded_length = len(tokens)\n",
    "    vocab_length = sum(len(token) for token in vocab)\n",
    "    compression_factor = (encoded_length + vocab_length) / total_length\n",
    "    return compression_factor\n",
    "\n",
    "def evaluate_vocab_size(genome, kmers, processed_genome_path, vocab_size, method=\"kmer\"):\n",
    "    kmer_file_path = os.path.join(subsampled_genomes_dir, f\"{org}_kmers.txt\")\n",
    "    save_kmers_to_file(kmers, kmer_file_path)\n",
    "    \n",
    "    with open(kmer_file_path, 'r') as f:\n",
    "        kmer_lines = f.readlines()\n",
    "        kmer_lines = [kmer.strip() for kmer in kmer_lines] \n",
    "        print(f\"{kmer_file_path} contains {len(kmer_lines)} kmers\")\n",
    "        print(f\"first 10 kmers: {kmer_lines[:10]}\")\n",
    "\n",
    "    model_prefix = os.path.join(tokenizers_dir, f\"{org}_{vocab_size}_{method}\")\n",
    "    print(f\"training w vocab size: {vocab_size}, model prefix: {model_prefix}\")\n",
    "    model_prefix = train_sentencepiece_tokenizer_kmer(kmer_file_path, model_prefix, vocab_size, method)\n",
    "\n",
    "    model_file = f\"{model_prefix}.model\"\n",
    "    if not os.path.exists(model_file):\n",
    "        raise FileNotFoundError(f\"model file not found: {model_file}\")\n",
    "\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(f\"{model_prefix}.model\")\n",
    "    tokens = sp.encode_as_pieces(genome)\n",
    "    \n",
    "    vocab = load_tokenizer_vocab(model_prefix)\n",
    "    compression_factor = calculate_compression_factor(genome, tokens, vocab)\n",
    "    print(f'Vocab size: {vocab_size}, compression factor: {compression_factor}, number of tokens: {len(tokens)}')\n",
    "    return (vocab_size, compression_factor, len(tokens))\n",
    "\n",
    "def extended_search_vocab_size(genome, kmers, processed_genome_path, initial_vocab_size=1000, step_size=1000, max_vocab_size=80000, threshold=1):\n",
    "    genome_length = len(genome)\n",
    "    if genome_length > 10**6:\n",
    "        step_size = 10000\n",
    "        max_vocab_size = 100000\n",
    "\n",
    "    vocab_size = initial_vocab_size\n",
    "    compression_factors = []\n",
    "    optimal_vocab_size = None\n",
    "\n",
    "    while vocab_size <= max_vocab_size:\n",
    "        result = evaluate_vocab_size(genome, kmers, processed_genome_path, vocab_size, method=\"kmer\")\n",
    "        if result:\n",
    "            vocab_size, compression_factor, num_tokens = result\n",
    "            compression_factors.append(result)\n",
    "\n",
    "            vocab_size += step_size\n",
    "\n",
    "    #find optimal vocab size based on the lowest compression factor\n",
    "    min_compression = min(compression_factors, key=lambda x: x[1])\n",
    "    optimal_vocab_size = min_compression[0]\n",
    "    \n",
    "    return compression_factors, optimal_vocab_size\n",
    "\n",
    "kmers = generate_kmers(genome, kmer_size)\n",
    "compression_factors, optimal_vocab_size = extended_search_vocab_size(\n",
    "    genome, kmers, processed_genome_path, initial_vocab_size=1000, step_size=1000, max_vocab_size=80000, threshold=1\n",
    ")\n",
    "\n",
    "if optimal_vocab_size is None:\n",
    "    print(f\"no optimal vocab size found. Compression factors: {compression_factors}\")\n",
    "else:\n",
    "    print(f\"Optimal vocab size for {org}: {optimal_vocab_size}\")\n",
    "\n",
    "def load_tokenizer(org, vocab_size, method=\"kmer\"):\n",
    "    model_path = os.path.join(tokenizers_dir, f\"{org}_{vocab_size}_{method}.model\")\n",
    "    kmer = spm.SentencePieceProcessor(model_file=model_path)\n",
    "    return kmer   \n",
    "\n",
    "kmer_model = load_tokenizer(org, optimal_vocab_size, method=\"kmer\")\n",
    "kmer_tokens = kmer_model.encode_as_pieces(genome)\n",
    "\n",
    "\n",
    "def plot_results(compression_factors):\n",
    "    vocab_sizes, compression_values, num_tokens = zip(*compression_factors)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(vocab_sizes, compression_values)\n",
    "    plt.xlabel('vocab size')\n",
    "    plt.ylabel('compression factor')\n",
    "    plt.title('compression factor vs vocab size')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(num_tokens, compression_values)\n",
    "    plt.xlabel('number of tokens')\n",
    "    plt.ylabel('compression factor')\n",
    "    plt.title('compression factor vs number of tokens')\n",
    "    plt.grid(True)\n",
    "    plt.xlim(left=0)\n",
    "    plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x):,}'))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_tokens(tokens):\n",
    "    tokcounts = Counter(tokens).most_common()\n",
    "    print(tokcounts[:30])\n",
    "\n",
    "    plt.hist([len(t[0]) for t in tokcounts],\n",
    "            bins=np.arange(0,max([len(t[0]) for t in tokcounts])+1))\n",
    "    plt.xlabel('token length')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.title('token length distribution')\n",
    "    plt.show()\n",
    "\n",
    "    TC = [t[1] for t in tokcounts]\n",
    "    plt.plot(np.arange(0, len(TC), 1), TC)\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('rank')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.show()    \n",
    "\n",
    "plot_results(compression_factors)\n",
    "analyze_tokens(kmer_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
